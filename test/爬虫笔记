模拟浏览器，抓取数据 爬虫
第一章:
第一节
第二节
第三节  爬虫使用
   1.使用场景分类
      - 通用爬虫
        抓取系统的重要组成部分。抓取的互联网中一整张页面数据。
      - 聚焦爬虫
        是建立在通用爬虫基础之上。抓取的是页面中特定局部内容。
      - 增量式爬虫
        检测网站中数据更新的情况。只会抓取网站中最新更新出来的数据。
    2.爬虫的矛与盾
      - 反爬机制
         门户网站可以通过指定相应的策略或者技术手段防止爬虫程序进行网站数据的爬取。

      -  反反爬策略
      - robots.txt 协议
         君子协定。
         www.taobao.com/robots.txt

第四节：http和https
        - 概念：服务器和客户进行数据交互的一种形式。
     1.常用请求头信息(只介绍在抓取中用到的信息)
       - User-Agent:请求载体的身份标示
       - Connection:请求完毕后，是断开链接还是保持链接。 keepalive or close.

     2.常用响应头信息
       - Content-Type: 服务器端响应回客户端的数据类型（字符串,json...）

     3.https协议
       - 安全的http协议
     4.加密方式
       - 对称密钥加密
       - 非对称密钥加密
       - 证书密钥加密

第二章：requests模块
基于网络请求的模块 urllib模块和requests模块（只讲requests模块）

第一节：requests模块：
    python中原声的网络请求模块，功能强大，简单便捷，高效。
    作用:模拟浏览器发请求。

    如何使用（requests模块的编码流程）：
       - 指定url
       - 发起http或https请求，方式可能是get或post请求。
       - 获取响应数据，
       - 持久化存储响应数据
    环境安装：
       pip install requests

    实战编码：
       - 需求:爬取sogo首页的网站数据。

第二节：requests实战
     需求:
        - 爬取搜狗指定此条对应的搜索结果页面
        - 破解百度翻译
        - 爬取豆瓣电影分类排行榜
        - 爬取肯定记餐厅查询 http://www.kfc.com.cn/kfccda/index.aspx
        - 爬取国家药品监督管理中局中基于中华人民共和国化妆品许可证相关数据


     1.爬取搜狗指定此条对应的搜索结果页面


     作业：

 第三章   数据解析
 第一节: 概述
    聚焦爬虫，爬取页面中指定的页面内容。
        - 指定url
        - 发起请求
        - 获取响应数据
        - 数据解析
        - 持久化存储

    - 基于正则实现的数据解析
    - bs4
    - xpath 重点（不止在python中可以，在其它语言中也可以）

    数据解析的原理：
    - 解析的局部文本内容都会在标签之间或者标签对应的属性中获取到
    - 1.进行指定标签的定位
    - 2.标签或者标签对应的属性中存储的数据值进行提取(解析)
 第二节:正则（1）
 单字符：
      . : 除换行以外所有字符
      []: [aoe] [a-w] 匹配集合中的任意一个字符
      \d: 数字 [0-9]
      \D: 非数字
      \w: 数字、字母、下划线、中文
      \W: 非\w
      \s:所有的空白字符，空格、制表符、换页符等等。等价于 [\f\n\r\t\v]
      \S: 非空白
 数量修饰符:
      *:任意多次   >=0
      +:至少一次 >=1
      ?:可有可无 0次或1次
      {m}:固定m次   hello{3,}
      {m,}:至少m次
      {m,n}:m-n次
 边界:
      $:以某某结尾
      ^:以某某开头
 分组:
     (ab)
 贪婪模式: .*
 非贪婪模式: .*?

 re.I  :忽略大小写
 re.M  :多行匹配
 re.S  : 单行匹配
 re.sub(正则表达式，替换内容，字符串)


 正则练习：
 import re
 key = "javapythonc++php"
 re.findall('python',key)[0]   #提取出python

 key="<html><h1>hello world</h1></html>"
 re.findall('<h1>(.*)</h1>',key)[0]   #提取出hello world

 string='我喜欢身高为170的女孩'
 re.findall('\d+',string)  #提取170

 key='http://www.baidu.com and https://boob.com'
 re.findall('https?://',key)    #提取出http://和https://

 key ='lalala<hTml>hello</HtMl>hahah'
 re.findall('<[Hh][Tt][mM][lL]>(.*)</[Hh][Tt][mM][lL]>',key)   #提取出hello

 key = 'bobo@hit.edu.com'
 re.findall('h.*?\.',key)     #匹配hit

 key = 'saas and sas and saaas'
 re.findall('sa{1,2}s',key)   #匹配sas和saas

 第三节 bs4
     - 数据解析的原理
         - 1.标签定位
         - 2.提取标签、标签属性中存储的数据值

     - bs4数据解析的原理:
         - 1.实力话一个BeautifulSoup对象，并且讲页面源码数据加载到该对象中
         - 2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取
     - 环境安装
         - pip install bs4
         - pip install lxml
     - 如何实力化BeautifulSoup对象
         - from bs4 import BeautifulSoup
         - 对象的实力话:
             - 1.将本地的html文档中的数据加载到该对象中
                  fp = open('.test.html','r',encoding='utf-8')
                  soup = BeautifulSoup(fp,'lxml')
             - 2.将互联网的html文档加载到对象中
                   page_text = response.text
                   soup = BeautifulSoup(page_text,'lxml')




